# Awesome Search Algorithms for Test-Time Scaling

## paper list
<li>
  <i><b>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</b></i>, Google DeepMind; Princeton
  <a href="https://arxiv.org/pdf/2305.10601" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--12-red" alt="arXiv 2023-12">
  </a>
</li>

<li>
  <i><b>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</b></i>, Microsoft Research Asia
  <a href="https://arxiv.org/abs/2406.03816" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>Reasoning with Language Model is Planning with World Model</b></i>, UC San Diego
  <a href="https://arxiv.org/abs/2305.14992" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--05-red" alt="arXiv 2023-05">
  </a>
</li>

<li>
  <i><b>AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training</b></i>, University College London
  <a href="https://arxiv.org/pdf/2309.17179" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--09-red" alt="arXiv 2023-09">
  </a>
</li>

<li>
  <i><b>Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning</b></i>, University of Maryland
  <a href="https://arxiv.org/abs/2410.06508" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--10-red" alt="arXiv 2024-10">
  </a>
</li>

<li>
  <i><b>Toward self-improvement of llms via imagination, searching, and criticizing.</b></i>, Tencent AI Lab
  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/5e5853f35164e434015716a8c2a66543-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/NeurIPS-2024-green" alt="NeurIPS 2024">
  </a>
</li>

<li>
  <i><b>Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning</b></i>, National University of Singapore
  <a href="https://arxiv.org/pdf/2405.00451?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--05-red" alt="arXiv 2024-05">
  </a>
</li>

<li>
  <i><b>Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</b></i>, Shanghai AI Lab
  <a href="https://arxiv.org/pdf/2406.07394?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>TREE SEARCH FOR LANGUAGE MODEL AGENTS</b></i>, Carnegie Mellon University
  <a href="https://arxiv.org/pdf/2407.01476?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--07-red" alt="arXiv 2024-07">
  </a>
</li>

<li>
  <i><b>Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models</b></i>, National University of Defense Technology
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34924" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/AAAI-2025-green" alt="AAAI 2025">
  </a>
</li>

<li>
  <i><b>Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</b></i>, National University of Singapore
  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/65a39213d7d0e1eb5d192aa77e77eeb7-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/NeurIPS-2023-green" alt="NeurIPS 2023">
  </a>
</li>

<li>
  <i><b>PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding</b></i>, CNRS, IRISA, Univ. Rennes 1
  <a href="https://arxiv.org/pdf/2109.13582" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2021--09-red" alt="arXiv 2021-09">
  </a>
</li>

<li>
  <i><b>Planning with Large Language Models for Code Generation</b></i>, MIT-IBM Watson AI Lab
  <a href="https://arxiv.org/pdf/2303.05510" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--03-red" alt="arXiv 2023-03">
  </a>
</li>

<li>
  <i><b>Don't throw away your value model! Value-Guided Monte-Carlo Tree Search decoding</b></i>, FAIR, Meta
  <a href="https://arxiv.org/pdf/2309.15028" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--09-red" alt="arXiv 2023-09">
  </a>
</li>

<li>
  <i><b>ARGS: Alignment as Reward-Guided Search</b></i>, University of Wisconsin–Madison
  <a href="http://arxiv.org/pdf/2402.01694" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--02-red" alt="arXiv 2024-02">
  </a>
</li>

<li>
  <i><b>When is Tree Search Useful for LLM Planning? It Depends on the Discriminator</b></i>, The Ohio State University
  <a href="https://arxiv.org/pdf/2402.10890" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--02-red" alt="arXiv 2024-02">
  </a>
</li>

<li>
  <i><b>Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS</b></i>, Texas A&M University
  <a href="https://arxiv.org/pdf/2402.03289" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--02-red" alt="arXiv 2024-02">
  </a>
</li>

<li>
  <i><b>Interpretable Contrastive Monte Carlo Tree Search Reasoning</b></i>, The University of Sydney
  <a href="https://arxiv.org/pdf/2410.01707" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--10-red" alt="arXiv 2024-10">
  </a>
</li>

<li>
  <i><b>Can Large Language Models Play Games? A Case Study of A Self-Play Approach</b></i>
  <a href="http://arxiv.org/pdf/2403.05632" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--03-red" alt="arXiv 2024-03">
  </a>
</li>

<li>
  <i><b>RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation</b></i>, Shanghai Jiao Tong University
  <a href="https://arxiv.org/pdf/2409.09584" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--09-red" alt="arXiv 2024-09">
  </a>
</li>

<li>
  <i><b>Verified multi-step synthesis using large language models and monte carlo tree search</b></i>, Harvard University
  <span>
    <img src="https://img.shields.io/badge/Preprint-2024--02-blue" alt="Preprint 2024-02">
  </span>
</li>

<li>
  <i><b>A novel approach to optimize large language models for named entity matching with monte carlo tree search</b></i>, Authorea Preprints
  <a href="https://d197for5662m48.cloudfront.net/documents/publicationstatus/224148/preprint_pdf/157c3392749a8fb55e09e2a452b5c1fb.pdf" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/Authorea-2024--09-blue" alt="Authorea 2024-09">
  </a>
</li>

<li>
  <i><b>Synthetic Data Generation from Real Data Sources using Monte Carlo Tree Search and Large Language Models</b></i>, Authorea Preprints
  <a href="https://www.authorea.com/doi/full/10.22541/au.172660477.72210139" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/Authorea-2024--01-blue" alt="Authorea 2024-01">
  </a>
</li>

<li>
  <i><b>Planning with Large Language Models for Conversational Agents</b></i>, Tianjin University
  <a href="https://papers.cool/arxiv/search?highlight=1&query=Planning+with+Large+Language+Models+for+Conversational+Agents&show=200" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/Preprint-2024--07-blue" alt="Preprint 2024-07">
  </a>
</li>

<li>
  <i><b>Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning</b></i>, Columbia University
  <span>
    <img src="https://img.shields.io/badge/Preprint-2024--10-blue" alt="Preprint 2024-10">
  </span>
</li>

<li>
  <i><b>VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search</b></i>, Kempner Institute at Harvard University
  <a href="https://arxiv.org/pdf/2402.08147" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--02-red" alt="arXiv 2024-02">
  </a>
</li>

<li>
  <i><b>KNOT-MCTS: Addressing Hallucinations in Generative Language Modeling for QA</b></i>, National Taiwan Normal University
  <a href="https://aclanthology.org/2023.rocling-1.26.pdf" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/RoCLING-2023-green" alt="ROCLING 2023">
  </a>
</li>

<li>
  <i><b>REX: Rapid Exploration and eXploitation for AI Agents</b></i>, Salesforce Research
  <a href="https://openreview.net/forum?id=8TAGx549Ns" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/OpenReview-2023--07-blue" alt="OpenReview 2023-07">
  </a>
</li>

<li>
  <i><b>Toolchain*: Efficient action space navigation in large language models with A* search</b></i>, Georgia Institute of Technology
  <a href="https://arxiv.org/pdf/2310.13227" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--10-red" alt="arXiv 2023-10">
  </a>
</li>

<li>
  <i><b>No train still gain: MCTS guided by energy function</b></i>, Independent Researcher
  <a href="https://arxiv.org/pdf/2309.03224" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--09-red" alt="arXiv 2023-09">
  </a>
</li>

<li>
  <i><b>Generating Code World Models with LLMs Guided by MCTS</b></i>, Aalto University
  <a href="https://arxiv.org/abs/2405.15383" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/NeurIPS-2024-green" alt="NeurIPS 2024">
  </a>
</li>

<li>
  <i><b>Agent Q: Advanced reasoning and learning for autonomous AI agents</b></i>, The AGI Company (MultiOn)
  <a href="https://arxiv.org/pdf/2408.07199?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--08-red" alt="arXiv 2024-08">
  </a>
</li>

<li>
  <i><b>Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers</b></i>, Microsoft Research Asia
  <a href="https://arxiv.org/pdf/2408.06195?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--08-red" alt="arXiv 2024-08">
  </a>
</li>

<li>
  <i><b>AlphaMath Almost Zero: Process Supervision without Process</b></i>, Tongyi Lab
  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/30dfe47a3ccbee68cffa0c19ccb1bc00-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/NeurIPS-2024-green" alt="NeurIPS 2024">
  </a>
</li>

<li>
  <i><b>Information Directed Tree Search: Reasoning and Planning with Language Agents</b></i>, Stanford
  <a href="https://openreview.net/pdf?id=turWYO1w2Q" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/Workshop-2024--07-blue" alt="Workshop 2024-07">
  </a>
</li>

<li>
  <i><b>CPL: Critical Planning Step Learning Boosts LLM Generalization</b></i>, Microsoft Research Asia
  <a href="https://arxiv.org/pdf/2409.08642?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--09-red" alt="arXiv 2024-09">
  </a>
</li>

<li>
  <i><b>Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning</b></i>, Skywork AI
  <a href="https://arxiv.org/pdf/2406.14283?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>RoT: Enhancing LLMs with Reflection on Search Trees</b></i>, ShanghaiTech
  <a href="https://arxiv.org/pdf/2404.05449?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--04-red" alt="arXiv 2024-04">
  </a>
</li>

<li>
  <i><b>LiteSearch: Efficacious Tree Search for LLM</b></i>, Xiamen University
  <a href="https://arxiv.org/pdf/2407.00320?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>Large language model guided tree-of-thought</b></i>, Theta Labs, Inc.
  <a href="https://arxiv.org/pdf/2305.08291" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--05-red" alt="arXiv 2023-05">
  </a>
</li>

<li>
  <i><b>Search-in-the-Chain</b></i>, Chinese Academy of Sciences
  <a href="https://dl.acm.org/doi/pdf/10.1145/3589334.3645363" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/WWW-2024-green" alt="WWW 2024">
  </a>
</li>

<li>
  <i><b>LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning</b></i>, UCLA
  <a href="https://arxiv.org/pdf/2407.02511" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>Monte Carlo Thought Search for Catalyst Design</b></i>, Pacific Northwest National Laboratory
  <a href="https://arxiv.org/pdf/2310.14420" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--10-red" alt="arXiv 2023-10">
  </a>
</li>

<li>
  <i><b>Strategist: Learning Strategic Skills via Bi-Level Tree Search</b></i>, Rensselaer Polytechnic Institute
  <a href="https://arxiv.org/pdf/2408.10635?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--08-red" alt="arXiv 2024-08">
  </a>
</li>

<li>
  <i><b>Language Agent Tree Search</b></i>
  <a href="https://arxiv.org/pdf/2310.04406" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--10-red" alt="arXiv 2023-10">
  </a>
</li>

<li>
  <i><b>LLM Reasoners: New Evaluation, Library, and Analysis</b></i>, UC San Diego
  <a href="https://arxiv.org/pdf/2404.05221" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--04-red" alt="arXiv 2024-04">
  </a>
</li>

<li>
  <i><b>PromptAgent: Strategic Planning with Language Models</b></i>, UC San Diego
  <a href="https://arxiv.org/pdf/2310.16427" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--10-red" alt="arXiv 2023-10">
  </a>
</li>

<li>
  <i><b>OVM: Outcome-supervised Value Models for Planning in Mathematical Reasoning</b></i>, CUHK Shenzhen
  <a href="https://arxiv.org/pdf/2311.09724" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--11-red" alt="arXiv 2023-11">
  </a>
</li>

<li>
  <i><b>MindStar: Enhancing math reasoning at inference time</b></i>, Noah’s Ark Laboratory
  <a href="https://arxiv.org/pdf/2405.16265?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--05-red" alt="arXiv 2024-05">
  </a>
</li>

<li>
  <i><b>Deductive Beam Search</b></i>, Fudan University
  <a href="https://arxiv.org/pdf/2401.17686" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--01-red" alt="arXiv 2024-01">
  </a>
</li>

<li>
  <i><b>Let’s reward step by step</b></i>, National University of Singapore
  <a href="https://arxiv.org/pdf/2310.10080" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--10-red" alt="arXiv 2023-10">
  </a>
</li>

<li>
  <i><b>Step-level Value Preference Optimization for Mathematical Reasoning</b></i>, Tongyi Lab
  <a href="https://arxiv.org/pdf/2406.10858" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>Improve Mathematical Reasoning by Automated Process Supervision</b></i>, Google DeepMind
  <a href="https://arxiv.org/pdf/2406.06592?" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2024--06-red" alt="arXiv 2024-06">
  </a>
</li>

<li>
  <i><b>Everything of Thoughts (XOT)</b></i>, Microsoft
  <a href="https://arxiv.org/pdf/2311.04254" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--11-red" alt="arXiv 2023-11">
  </a>
</li>

<li>
  <i><b>Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning</b></i>, Department of Computer Science, Columbia University
  <a href="https://arxiv.org/pdf/2305.13660" target="_blank" rel="noopener noreferrer">
    <img src="https://img.shields.io/badge/arXiv-2023--05-red" alt="arXiv 2023-05">
  </a>
</li>


# 这是之前添加的
## Uniform-Search

**Tree of Thoughts: Deliberate Problem Solving with Large Language Models**, Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan. [pdf]([link_to_pdf](https://arxiv.org/abs/2305.10601))
   
   Search Algorithm: **DFS**, **BFS**


**Self-Evaluation Guided Beam Search for Reasoning**

   Search ALgorithm: **Beam Search**


## Informed (Heuristic) Search Algorithms

### A*

###




https://arxiv.org/abs/2406.03816

### MCTS 

**ReST-MCTS∗: LLM Self-Training via Process Reward Guided Tree Search**, Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang. [pdf]([link_to_pdf](https://arxiv.org/abs/2406.03816))

Search Algorithm: **MCTS**


**Reasoning with Language Model is Planning with World Model**

AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training

Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning

Toward self-improvement of llms via imagination, searching, and criticizing.

Monte carlo tree search boosts reasoning via iterative preference learning.

Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B

TREE SEARCH FOR LANGUAGE MODEL AGENTS


Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models

Large Language Models as Commonsense Knowledge for Large-Scale Task Planning

PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding

Planning with Large Language Models for Code Generation

Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding

ARGS: Alignment as reward-guided search

When is Tree Search Useful for LLM Planning? It Depends on the Discriminator

Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS

Interpretable Contrastive Monte Carlo Tree Search Reasoning

Can Large Language Models Play Games? A Case Study of A Self-Play Approach

RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation



Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models

Verified multi-step synthesis using large language models and monte carlo tree search

A novel approach to optimize large language models for named entity matching with monte carlo tree search

Synthetic Data Generation from Real Data Sources using Monte Carlo Tree Search and Large Language Models

Planning with Large Language Models for Conversational Agents

Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning
Improve Mathematical Reasoning in Language Models by Automated Process Supervision

VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search

KNOT-MCTS: An Effective Approach to Addressing Hallucinations in Generative Language Modeling for Question Answering

REX: Rapid Exploration and eXploitation for AI Agents

SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning

Toolchain*: Efficient action space navigation in large language models with a* search

No train still gain. unleash mathematical reasoning of large language models with monte carlo tree search guided by energy function

Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search

Agent q: Advanced reasoning and learning for autonomous ai agents

Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers

AlphaMath Almost Zero: Process Supervision without Process

Information Directed Tree Search: Reasoning and Planning with Language Agents

CPL: Critical Planning Step Learning Boosts LLM Generalization in Reasoning Tasks

Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning

RoT: Enhancing Large Language Models with Reflection on Search Trees

LiteSearch: Efficacious Tree Search for LLM

Large language model guided tree-of-thought

Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks

LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning

Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design

Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search

Language agent tree search unifies reasoning acting and planning in language models

Markov Chain of Thought for Efficient Mathematical Reasoning


## 1130 ***
**LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models**

```
!!: PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization
```


Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models

OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning

Mindstar: Enhancing math reasoning in pre-trained llms at inference time

Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning

Let’s reward step by step: Step-level reward model as the navigators for reasoning


## 1202
Planning In Natural Language Improves LLM Search For Code Generation

**LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search**

## 1204

**o1-Coder: an o1 Replication for Coding**

Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS

SRA-MCTS: Self-driven Reasoning Aurmentation with Monte Carlo Tree Search for Enhanced Code Generation

**RuAG: Learned-rule-augmented Generation for Large Language Models**

SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement

DAWN-ICL: Strategic Planning of Problem-solving Trajectories for Zero-Shot In-Context Learning

**SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning**

**MulBerry: Empowering MLLM with o1-like reasoning**

## 0108

**BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**

**RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement**

**Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions** 

**Step-level Value Preference Optimization for Mathematical Reasoning**

**Improve Mathematical Reasoning in Language Models by Automated Process Supervision**  Google DeepMind

**Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation**

RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models

CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation

SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement

Progressive Multimodal Reasoning via Active Retrieval

Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking

PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion

Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration

What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning

Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning

Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling

RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs 

Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search

SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search

WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration

A Training Data Recipe to Accelerate A* Search with Language Models 

Planning Like Human: A Dual-process Framework for Dialogue Planning

Graph of thoughts: Solving elaborate problems with large language models

KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection

Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning

# 0110

**rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking**
